{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/automated-fact-checking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/drchajan/devel/python/FC/aic-nlp-utils/aic_nlp_utils/json.py:2: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, OrderedDict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import textwrap\n",
    "from tqdm import tqdm\n",
    "\n",
    "%cd /home/drchajan/devel/python/FC/automated-fact-checking\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from aic_nlp_utils.json import read_jsonl, read_json, write_json, write_jsonl\n",
    "from aic_nlp_utils.encoding import nfc\n",
    "from aic_nlp_utils.fever import fever_detokenize\n",
    "\n",
    "from factsearch.mark import EmphasizeClaimWords\n",
    "\n",
    "# from evaluation.document_retrieval import collect_eval_data, retriever_score\n",
    "# from evaluation.nli import LABEL_NUM, LABEL_STR, load_rte_model, evaluate_examples, evaluate_all_nli\n",
    "# from evaluation.fcheck_pipeline import read_fever_db, collect_scores_full, evaluate_full, extract_texts_ctk, InputSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original not calibrated versions (BAD, but does not matter for RND)\n",
    "# PVI_DIR_CS = '/mnt/data/factcheck/wiki/cs/20230801/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/pvi'\n",
    "# PVI_DIR_FCS= '/mnt/data/factcheck/NLI/csfever_nli_cls/pvi'\n",
    "# PVI_DIR_EN = '/mnt/data/factcheck/wiki/en/20230801/qacg/nli/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/pvi'\n",
    "# PVI_DIR_FEN = '/mnt/data/factcheck/NLI/nli_fever_cls/pvi'\n",
    "\n",
    "PVI_DIR_CS = '/mnt/data/factcheck/wiki/cs/20230801/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/pvi_calibrated'\n",
    "PVI_DIR_FCS= '/mnt/data/factcheck/NLI/csfever_nli_cls/pvi_calibrated'\n",
    "PVI_DIR_EN = '/mnt/data/factcheck/wiki/en/20230801/qacg/nli/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/pvi_calibrated'\n",
    "PVI_DIR_FEN = '/mnt/data/factcheck/NLI/nli_fever_cls/pvi_calibrated'\n",
    "PVI_DIR_CS_FS = \"/mnt/data/factcheck/wiki/cs/20230801/qacg/nli/PAV-ner-CNEC/mt5-large_all-cp126k/mt5-large_all-cp156k/pvi_fever_size_calibrated\"\n",
    "PVI_DIR_EN_FS = \"/mnt/data/factcheck/wiki/en/20230801/qacg/nli/stanza/mt5-large_all-cp126k/mt5-large_all-cp156k/pvi_fever_size_calibrated\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total claims: 400, label counts: Counter({'SUP': 200, 'REF': 200})\n"
     ]
    }
   ],
   "source": [
    "def sample_blind_test(cfg, n=100, seed=1235):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    rng = np.random.RandomState(seed)\n",
    "    claim2evidence_sum = {}\n",
    "    for name, claim2evidence_file in cfg.items():\n",
    "        claim2evidence = read_json(claim2evidence_file)\n",
    "        cids = list(claim2evidence.keys())\n",
    "        labels = [v[0][\"target\"] for v in claim2evidence.values()]\n",
    "        cid2label = {cid: label for cid, label in zip(cids, labels)}\n",
    "        selected_cids, remaining_cids = train_test_split(cids, train_size=n, random_state=rng, stratify=labels)\n",
    "        for cid in selected_cids:\n",
    "            cidnew = f\"{name}:{cid}\"\n",
    "            claim2evidence_sum[cidnew] = claim2evidence[cid]\n",
    "            for e in claim2evidence_sum[cidnew]:\n",
    "                e[\"claim_id\"] = cidnew\n",
    "    cids = list(claim2evidence_sum.keys())\n",
    "    rng.shuffle(cids)\n",
    "    claim2evidence_sum = {cid: claim2evidence_sum[cid] for cid in cids}\n",
    "    cnt_labels = Counter([v[0][\"target\"] for v in claim2evidence_sum.values()])\n",
    "    print(f\"total claims: {len(claim2evidence_sum)}, label counts: {cnt_labels}\")\n",
    "    return claim2evidence_sum\n",
    "\n",
    "\n",
    "# DIR = \"fever2fullwiki\" # FEVER testing, QACG-SUM model, QACG corpora\n",
    "# DIR = \"feverfever2fullwiki\" # FEVER testing, FEVER model, QACG corpora\n",
    "DIR = \"fullwiki2fullwiki\" # QACG testing, QACG-SUM model, QACG corpora\n",
    "# DIR = \"fullwikifever2fullwiki\" # QACG testing, FEVER model, QACG corpora\n",
    "\n",
    "# LANG = \"cs\"\n",
    "LANG = \"en\"\n",
    "claim2evidence_files = {\n",
    "    \"anserini\": f\"data/ncaa/er_annotations/{DIR}/{LANG}_claim2evidence_anserini.json\",\n",
    "    \"colbert\":  f\"data/ncaa/er_annotations/{DIR}/{LANG}_claim2evidence_colbert.json\",\n",
    "    \"colbert_ans\": f\"data/ncaa/er_annotations/{DIR}/{LANG}_claim2evidence_colbert_anserini_filtered.json\",\n",
    "    \"colbert_nli\": f\"data/ncaa/er_annotations/{DIR}/{LANG}_claim2evidence_colbert_nli_sorted.json\"\n",
    "}\n",
    "\n",
    "if DIR in [\"feverfever2fullwiki\", \"fullwikifever2fullwiki\"]:\n",
    "    del claim2evidence_files[\"anserini\"] # already computed for fever2fullwiki\n",
    "    \n",
    "claim2evidence = sample_blind_test(claim2evidence_files, n=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'claim': 'Bible Quiz is used by the WBQA.',\n",
       "  'context': 'Bible quiz\\n\\nIn WBQA quizzing, questions must \"only\" contain words from the verse from which the question is taken, plus an interrogative (who/what/where/when/why/how) and, if necessary, a form of the verb \"to be\" (i.e. was/is/were/am, though in practice very few questions require this addition)--no other helping verbs may be used. All questions must be grammatically correct (with the exception of questions beginning with \"what if\"). This is the same style of question used in other groups such as Bible Quiz Fellowship.\\n\\nBible quiz\\n\\nWBQA operates in close cooperation with numerous independent local leagues in the Great Lakes region, such as Detroit Bible Quizzing, Huron Valley Bible Quizzing in the Ann Arbor area, the Ohio Bible Quizzing Association centered around Chillicothe, the Shenango Valley Bible Quiz league around the Pittsburgh area, BIC (Brethren in Christ) Quizzing in the Harrisburg region, and Rochester Youth for Christ Quizzing around Rochester, Minnesota.\\n\\nWBQA quizzing held a “World finals” tournament in summer from 1974 to 2010, usually at a midwestern Christian college or university. Champions nearly always came from the northeast United States, primarily the Pittsburgh area, northeast Ohio, central Pennsylvania, and Maine. The Detroit program enjoyed a period of dominance in the tournament\\'s latter years and remains one of the largest and most competitive WBQA-affiliated organizations today. The “World finals” tournament is now held annually in the Chicago area each year in early April, and registration is open to any team. Other major tournaments include the Great Lakes Regional Tournament in November, and the All-Timers tournament, open to both children and adults, in January.',\n",
       "  'claim_id': 'colbert_nli:2946',\n",
       "  'bid': 'Bible_quiz_22',\n",
       "  'probs': {'NEI': '0.24321686', 'REF': '0.25850397', 'SUP': '0.49827918'},\n",
       "  'pred': 'SUP',\n",
       "  'target': 'SUP',\n",
       "  'rank': 2,\n",
       "  'kw_rank': 1},\n",
       " {'claim': 'Bible Quiz is used by the WBQA.',\n",
       "  'context': 'Bible quiz\\n\\nBQF-style Bible Quizzing is a fast-paced team sport, in which 3 teams of 5-7 people compete to answer questions based directly on scripture verses. Bible Quiz Fellowship uses a modified version of the WBQA\\'s 8-year cycle through the New Testament. In 2010, the 8-year cycle was changed to incorporate Mark and Revelation, so that the cycle now includes the entire New Testament.\\n\\nBible quiz\\n\\nEach quiz has two six-minute halves, during which the quizmaster asks as many questions as time allows (usually around 40–50). Approximately 15% of questions are quote questions, for which the reference is given, and the verse must be quoted correctly to get points. The rest of the questions are reference-type questions. Each correct answer is worth 20 points, and points are deducted from the team score, but not individual score, for errors. Five correct answers is a \"quiz-out,\" and three errors are an \"error-out\"; a ten-point bonus is awarded for a \"perfect quiz-out\", when a quizzer gets five right and none wrong in a quiz. A 20-point bonus is awarded for the 5th, 6th and 7th quizzers answering correctly',\n",
       "  'claim_id': 'colbert_nli:2946',\n",
       "  'bid': 'Bible_quiz_11',\n",
       "  'probs': {'NEI': '0.09051542', 'REF': '0.4703853', 'SUP': '0.43909925'},\n",
       "  'pred': 'REF',\n",
       "  'target': 'SUP',\n",
       "  'rank': 1,\n",
       "  'kw_rank': 3}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(claim2evidence.values())[0][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from portion import closedopen\n",
    "from ufal.morphodita import Forms, TokenRanges, Tokenizer\n",
    "# from ufal.morphodita import Tokenizer_newCzechTokenizer, Tokenizer_newEnglishTokenizer, Tokenizer_newGenericTokenizer, Tokenizer_newVerticalTokenizer\n",
    "\n",
    "class MorphoDiTaTokenizer:\n",
    "    def __init__(self, lang:str =\"cs\"):\n",
    "        lang = lang.lower()\n",
    "        assert lang in [\"cs\", \"en\", \"generic\", \"vertical\"]\n",
    "        if lang == \"cs\":\n",
    "            # self.tokenizer = Tokenizer_newCzechTokenizer()\n",
    "            self.tokenizer = Tokenizer.newCzechTokenizer()\n",
    "        elif lang == \"en\":\n",
    "            # self.tokenizer = Tokenizer_newCzechTokenizer()\n",
    "            self.tokenizer = Tokenizer.newCzechTokenizer()\n",
    "        elif lang == \"generic\":\n",
    "            # self.tokenizer = Tokenizer_newGenericTokenizer()\n",
    "            self.tokenizer = Tokenizer.newGenericTokenizer()\n",
    "        elif lang == \"vertical\":\n",
    "            # self.tokenizer = Tokenizer_newVerticalTokenizer()\n",
    "            self.tokenizer = Tokenizer.newVerticalTokenizer()\n",
    "        self.forms = Forms()\n",
    "        self.tokens = TokenRanges()\n",
    "\n",
    "\n",
    "    def tokenizeSentences(self, text: str, spans: bool=False):\n",
    "        self.tokenizer.setText(text)\n",
    "        while self.tokenizer.nextSentence(self.forms, self.tokens):\n",
    "            first = self.tokens[0].start\n",
    "            last = self.tokens[-1].start + self.tokens[-1].length\n",
    "            if spans:\n",
    "                yield text[first:last], closedopen(first, last)\n",
    "            else:\n",
    "                yield text[first:last]\n",
    "\n",
    "    def tokenizeWords(self, text: str, spans: bool=False):\n",
    "        self.tokenizer.setText(text)\n",
    "        while self.tokenizer.nextSentence(self.forms, self.tokens):\n",
    "            for form, token in zip(self.forms, self.tokens):\n",
    "                if spans:\n",
    "                    first = token.start\n",
    "                    last = token.start + token.length\n",
    "                    yield form, closedopen(first, last)\n",
    "                else:\n",
    "                    yield form\n",
    "\n",
    "class StopWordList:\n",
    "    def __init__(self, fname=\"/home/drchajan/devel/python/FC/drchajan/data/stopwords/czech.txt\"):\n",
    "        with open(fname) as f:\n",
    "            self.stopwords = set([l.strip().lower() for l in f.readlines()])\n",
    "\n",
    "    def is_stopword(self, word):\n",
    "        return word.lower() in self.stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jaro import jaro_winkler_metric\n",
    "import portion as P\n",
    "import re\n",
    "\n",
    "from utils.stopwords import StopWordList\n",
    "from utils.tokenization import MorphoDiTaTokenizer\n",
    "\n",
    "def unify_spans(spanrecs):\n",
    "    # reduces the number of spans by computing union of all spans - any span inside a larger one is removed\n",
    "    spanrecs = spanrecs.copy()\n",
    "    intervals = [s[\"span\"] for s in spanrecs[\"spans\"]]\n",
    "    if len(intervals) == 0:\n",
    "        return spanrecs\n",
    "    assert len(spanrecs[\"spans\"][0].keys()) == 1, \"more information beyond `span` not supported yet!\"\n",
    "    union = intervals[0]\n",
    "    for interval in intervals[1:]:\n",
    "        union = union | interval\n",
    "    # split union to the intervals\n",
    "    spanrecs[\"spans\"] = [{\"span\": interval} for interval in union] \n",
    "    return spanrecs\n",
    "\n",
    "\n",
    "def emphasize_claim_words_jaro_winkler(claim, doc, min_chars=3, jaro_winkler_threshold=0.8, stopwordlist=\"/home/drchajan/devel/python/FC/drchajan/\", lang=\"cs\"):\n",
    "    # returns list of `doc` words which should be emphasised w.r.t. the `claim`\n",
    "    tokenizer = MorphoDiTaTokenizer(lang=lang)\n",
    "    stopwordlist = StopWordList(stopwordlist)\n",
    "    claim_words = set([w.lower() for w in tokenizer.tokenizeWords(claim) if (not stopwordlist.is_stopword(w)) and len(w) >= min_chars])\n",
    "    doc_words = set([w.lower() for w in tokenizer.tokenizeWords(doc) if (not stopwordlist.is_stopword(w)) and len(w) >= min_chars])\n",
    "    emp_words = set()\n",
    "    for cw in claim_words:\n",
    "        for dw in doc_words:\n",
    "            dist = jaro_winkler_metric(cw, dw)\n",
    "#             print(cw, dw, dist)\n",
    "            if dist >= jaro_winkler_threshold:\n",
    "                emp_words.add(dw)\n",
    "            \n",
    "    return emp_words\n",
    "\n",
    "def emphasize_claim_words_jaro_winkler_spans(claim, doc, min_chars=3, jaro_winkler_threshold=0.8, stopwordlist=\"/home/drchajan/devel/python/FC/drchajan/data/stopwords/czech.txt\", lang=\"cs\"):\n",
    "    # find words to emphasize\n",
    "    emp_words = emphasize_claim_words_jaro_winkler(claim, doc, min_chars=min_chars, jaro_winkler_threshold=jaro_winkler_threshold, stopwordlist=stopwordlist, lang=lang)\n",
    "    # find them in `doc` and return the spans\n",
    "    spans = []\n",
    "    for w in emp_words:\n",
    "        spans += [{\"span\": P.closedopen(a.start(), a.end())} for a in re.finditer(w, doc, re.IGNORECASE)]\n",
    "    \n",
    "    return unify_spans({\"type\": \"claim_words\", \"spans\": spans})\n",
    "\n",
    "claim = \"Food Network is offered to 83.3% of homes with a television.\"\n",
    "doc = \"\"\"Oprah Winfrey Network American pay television network Oprah Winfrey Network (OWN) is an American multinational basic cable channel jointly owned by Warner Bros. Discovery and Harpo Studios that launched on January 1, 2011, replacing the Discovery Health Channel. Oprah Winfrey Network The network is led by talk show host and namesake Oprah Winfrey and largely features entertainment and lifestyle programming targeting African American audiences, and reruns of talk show programming from the Harpo Studios library (including \"The Oprah Winfrey Show\"). Initially a 50/50 joint venture, Discovery acquired a larger stake in the network in 2017 and again in December 2020, when Discovery increased its ownership in OWN from 73% to 95%. Harpo remains a \"significant\" minority stakeholder and Winfrey is contracted with the channel through at least 2025. Oprah Winfrey Network As of February 2015, OWN is available to approximately 81.9 million pay television households (70.3% of households with television) in the United States. History. Development. After becoming Discovery Communications' new CEO in 2007, David Zaslav found Discovery Health to be underperforming along with its other digital cable networks launched in the last decade, and taking in significantly lower carriage fees in comparison to the company's namesake, Discovery Channel. As a result, he began to explore the possibility of re-launching the channel as a joint venture with another partner. Zaslav's wife was an avid reader of Oprah Winfrey's \"O\" magazine (a joint venture with Hearst Corporation); believing that her values could serve as the basis for a cable network, he contacted Winfrey's agents to hold a meeting in April 2007. On January 15, 2008, Discovery Communications officially announced that it had entered into a joint venture with Winfrey's studio Harpo Productions, under which it would re-launch Discovery Health as \"OWN: The Oprah Winfrey Network\", in the second half of 2009.\"\"\"\n",
    "\n",
    "def apply_marks(txt, spans):\n",
    "    if len(spans) == 0:\n",
    "        return txt\n",
    "\n",
    "    # type2color = {\"claim_words\": \"#FADBD8\", \"importance_sentence\": \"#fcf8e3\", \"importance_word\": \"#fcf8e3\"}\n",
    "    col = '#FADBD8' # for black background\n",
    "    # col = '#4F500E' # for black background\n",
    "    markedtxt = []\n",
    "    lold = 0\n",
    "    for span in spans:\n",
    "        f, l = span[\"span\"].lower, span[\"span\"].upper\n",
    "        # col = type2color[span[\"type\"]]\n",
    "        markedtxt.append(txt[lold:f])\n",
    "        markedtxt.append(f'<span style=\"padding:0 0.1em;background-color:{col}\">{txt[f:l]}</span>')\n",
    "        lold = l\n",
    "    markedtxt.append(txt[lold:])\n",
    "    \n",
    "    return ''.join(markedtxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oprah Winfrey <span style=\"padding:0 0.1em;background-color:#FADBD8\">Network</span> American pay <span style=\"padding:0 0.1em;background-color:#FADBD8\">television</span> <span style=\"padding:0 0.1em;background-color:#FADBD8\">network</span> Oprah Winfrey <span style=\"padding:0 0.1em;background-color:#FADBD8\">Network</span> (OWN) is an American multinational basic cable channel jointly owned by Warner Bros. Discovery and Harpo Studios that launched on January 1, 2011, replacing the Discovery Health Channel. Oprah Winfrey <span style=\"padding:0 0.1em;background-color:#FADBD8\">Network</span> The <span style=\"padding:0 0.1em;background-color:#FADBD8\">network</span> is led by talk show host and namesake Oprah Winfrey and largely features entertainment and lifestyle programming targeting African American audiences, and reruns of talk show programming from the Harpo Studios library (including \"The Oprah Winfrey Show\"). Initially a 50/50 joint venture, Discovery acquired a larger stake in the <span style=\"padding:0 0.1em;background-color:#FADBD8\">network</span> in 2017 and again in December 2020, when Discovery increased its ownership in OWN from 73% to 95%. Harpo remains a \"significant\" minority stakeholder and Winfrey is contracted with the channel through at least 2025. Oprah Winfrey <span style=\"padding:0 0.1em;background-color:#FADBD8\">Network</span> As of February 2015, OWN is available to approximately 81.9 million pay <span style=\"padding:0 0.1em;background-color:#FADBD8\">television</span> households (70.3% of households with <span style=\"padding:0 0.1em;background-color:#FADBD8\">television</span>) in the United States. History. Development. After becoming Discovery Communications\\' <span style=\"padding:0 0.1em;background-color:#FADBD8\">new</span> CEO in 2007, David Zaslav <span style=\"padding:0 0.1em;background-color:#FADBD8\">found</span> Discovery Health to be underperforming along with its other digital cable <span style=\"padding:0 0.1em;background-color:#FADBD8\">networks</span> launched in the last decade, and taking in significantly lower carriage fees in comparison to the company\\'s namesake, Discovery Channel. As a result, he began to explore the possibility of re-launching the channel as a joint venture with another partner. Zaslav\\'s wife was an avid reader of Oprah Winfrey\\'s \"O\" magazine (a joint venture with Hearst Corporation); believing that her values could serve as the basis for a cable <span style=\"padding:0 0.1em;background-color:#FADBD8\">network</span>, he contacted Winfrey\\'s agents to hold a meeting in April 2007. On January 15, 2008, Discovery Communications officially announced that it had entered into a joint venture with Winfrey\\'s studio Harpo Productions, under which it would re-launch Discovery Health as \"OWN: The Oprah Winfrey <span style=\"padding:0 0.1em;background-color:#FADBD8\">Network</span>\", in the second half of 2009.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if LANG == \"cs\":\n",
    "    spans = emphasize_claim_words_jaro_winkler_spans(claim, doc, jaro_winkler_threshold=0.8, stopwordlist=\"data/stopwords/cs.txt\", lang=LANG)\n",
    "elif LANG == \"en\":\n",
    "    spans = emphasize_claim_words_jaro_winkler_spans(claim, doc, jaro_winkler_threshold=0.8, stopwordlist=\"data/stopwords/en.txt\", lang=LANG)\n",
    "apply_marks(doc, spans[\"spans\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "\n",
    "def sample_to_html(sample, lang=\"cs\"):\n",
    "    assert lang in [\"cs\", \"en\"]\n",
    "    htmls = []\n",
    "    sup = f\"{100*float(sample['probs']['SUP']):.2f}%\"\n",
    "    ref = f\"{100*float(sample['probs']['REF']):.2f}%\"\n",
    "    nei = f\"{100*float(sample['probs']['NEI']):.2f}%\"\n",
    "    cls = sample['pred']\n",
    "    tgt = sample['target']\n",
    "    bid = sample[\"bid\"]\n",
    "    rank = sample[\"rank\"]\n",
    "    kw_rank = sample.get(\"kw_rank\")\n",
    "\n",
    "    claim = sample[\"claim\"]\n",
    "    context = sample[\"context\"]\n",
    "    if lang == \"cs\":\n",
    "        spans = emphasize_claim_words_jaro_winkler_spans(claim, context, jaro_winkler_threshold=0.8)\n",
    "    elif lang == \"en\":\n",
    "        spans = emphasize_claim_words_jaro_winkler_spans(claim, context, jaro_winkler_threshold=0.8, min_chars=2, stopwordlist=\"/home/drchajan/devel/python/FC/drchajan/data/stopwords/english_nltk.txt\", lang=\"en\")\n",
    "    context = apply_marks(context, spans[\"spans\"])\n",
    "\n",
    "    htmls.append(f'<h1>{claim}</h1>')\n",
    "    # htmls.append(f'<p style=\"font-family:monospace;font-size:18px\">#{rank} ({kw_rank}) {bid}</p>')\n",
    "    htmls.append(f'<p style=\"font-family:monospace;font-size:18px\">{bid}</p>')\n",
    "    htmls.append(f'<p style=\"font-family:helvetica;font-size:18px\">target: {tgt} / prediction: {cls}<br/> SUP: {sup}, REF: {ref}, NEI: {nei}</p>')\n",
    "    htmls.append(f'<p style=\"font-family:serif;font-size:20px;text-align:justify\">{context}</p>')\n",
    "    html = '\\n'.join(htmls)\n",
    "    return html\n",
    "\n",
    "def print_predictions(claim2evidence, min_confidence=0.0):\n",
    "    htmls = []\n",
    "    for evidence_list in list(claim2evidence.values())[10:11]:\n",
    "        selected_evidence = [e for e in evidence_list if 100*np.max(e[\"probs\"]) >= min_confidence]\n",
    "        for sample in selected_evidence[:2]:\n",
    "            htmls.append(sample_to_html(sample))\n",
    "            htmls.append(f'<hr/>')\n",
    "    html = '\\n'.join(htmls)\n",
    "    display(HTML(html))\n",
    "\n",
    "# print_predictions(claim2evidence_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b0f94b8fc44e92a19de98378db330e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h1>Bible Quiz is used by the WBQA.</h1>\\n<p style=\"font-family:monospace;font-size…"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from ipywidgets import Button, HTML, VBox, HBox\n",
    "from datetime import datetime\n",
    "\n",
    "def annotate(claim2evidence, dst_jsonl, l=2, lang=\"cs\"):\n",
    "    dst_jsonl = Path(dst_jsonl)\n",
    "    samples = []\n",
    "    for evidence_list in claim2evidence.values():\n",
    "        for sample in evidence_list[:l]:\n",
    "            samples.append(sample)\n",
    "\n",
    "    recs = []\n",
    "    if dst_jsonl.is_file():\n",
    "        recs = read_jsonl(dst_jsonl)\n",
    "        for s, r in zip(samples, recs):\n",
    "            assert  s['bid'] == r['bid'] and s['rank'] == r['rank'], \"Existing annotation file does not match data!\" \n",
    "            # assert s['claim_id'] == r['claim_id'] and s['bid'] == r['bid'] and s['rank'] == r['rank'], \"Existing annotation file does not match data!\" \n",
    "    idx = len(recs)\n",
    "\n",
    "    html = HTML(sample_to_html(samples[idx], lang=lang))\n",
    "    btn_ok = Button(description='Ok', icon='thumbs-up')\n",
    "    btn_bad = Button(description='Bad', icon='ban')\n",
    "    btn_prev = Button(description='Previous', icon='backward')\n",
    "    count_html = HTML()\n",
    "\n",
    "    def show_count():\n",
    "        count_html.value = f\"{idx+1}/{len(samples)}\"\n",
    "    \n",
    "    show_count()\n",
    "    \n",
    "\n",
    "    def save_annotation(label):\n",
    "        nonlocal idx\n",
    "        nonlocal recs\n",
    "        date = datetime.now().strftime(\"%y%m%d_%H%M%S\")\n",
    "        s = samples[idx]\n",
    "        rec = {\"claim_id\": s[\"claim_id\"], \"bid\": s[\"bid\"], \"rank\": s[\"rank\"], \"kw_rank\": s.get(\"kw_rank\"), \"label\": label, \"date\": date}\n",
    "        idx += 1\n",
    "        recs.append(rec)\n",
    "        write_jsonl(dst_jsonl, recs)\n",
    "        if idx < len(samples):\n",
    "            s = samples[idx]\n",
    "            html.value = sample_to_html(s, lang=lang)\n",
    "            show_count()\n",
    "\n",
    "\n",
    "    def btn_ok_eventhandler(obj):\n",
    "        save_annotation(\"ok\")\n",
    "\n",
    "    def btn_bad_eventhandler(obj):\n",
    "        save_annotation(\"bad\")\n",
    "\n",
    "    def btn_prev_eventhandler(obj):\n",
    "        nonlocal idx\n",
    "        nonlocal recs\n",
    "        if len(recs) > 0:\n",
    "            idx -= 1\n",
    "            recs = recs[:-1]\n",
    "            s = samples[idx]\n",
    "            html.value = sample_to_html(s, lang=lang)\n",
    "            show_count()\n",
    "\n",
    "    btn_ok.on_click(btn_ok_eventhandler)\n",
    "    btn_bad.on_click(btn_bad_eventhandler)\n",
    "    btn_prev.on_click(btn_prev_eventhandler)\n",
    "\n",
    "    form = VBox([html, HBox([btn_ok, btn_bad, HTML(\"&nbsp;\"*30), btn_prev, count_html])])\n",
    "    return form\n",
    "\n",
    "# anotace z hlediska kvality evidence retrieval - muze dany clanek pomoci potvrdit/vyvratit claim?\n",
    "# wrongly formed claims are still acceptable (if understandable)\n",
    "# i dokument ktery by vysvetlit pravdepodobnou zamenu v claimu je ok\n",
    "# beru leading title, pripadne sekci jako \"personal life\" pokud bych tam hledal...\n",
    "\n",
    "# annotate(claim2evidence, f\"data/ncaa/er_annotations/{DIR}/{LANG}_annotations.jsonl\", l=2, lang=LANG)\n",
    "annotate(claim2evidence, f\"data/ncaa/er_annotations/{DIR}/{LANG}_annotations_test.jsonl\", l=2, lang=LANG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_stats(model2cfg, return_df=True, ignore_leading=False):\n",
    "    results = []\n",
    "\n",
    "    for model, cfg in model2cfg.items():\n",
    "        annotation_file = cfg[\"file\"]\n",
    "        res = read_jsonl(annotation_file)\n",
    "        par1fraction = 100.0 * len([1 for r in res if r['bid'].endswith(\"_1\")])/len(res)\n",
    "\n",
    "        claim2labels = defaultdict(list)\n",
    "\n",
    "        # documents for single claims must by sorted by the rank!\n",
    "        for r in res:\n",
    "            if (not ignore_leading) or (not r[\"bid\"].endswith(\"_1\")):\n",
    "                claim2labels[r[\"claim_id\"]].append(r[\"label\"])\n",
    "\n",
    "        method2mrr = defaultdict(list)\n",
    "        method2p = defaultdict(list)\n",
    "        method2failed1 = defaultdict(list)\n",
    "        method2failed2 = defaultdict(list)\n",
    "        method2bothok = defaultdict(list)\n",
    "        method2bothfail = defaultdict(list)\n",
    "\n",
    "        for cid, labels in claim2labels.items():\n",
    "            method = cid.split(\":\")[0]\n",
    "            \n",
    "            assert len([1 for l in labels if l not in ['ok', 'bad']]) == 0, labels\n",
    "            if (not ignore_leading) and len(labels) != 2:\n",
    "                # just for this experiment!\n",
    "                print(f\"skipping for cid: {cid}!\")\n",
    "                continue\n",
    "            \n",
    "            mrr = 0.0 # no relevant\n",
    "            for i, label in enumerate(labels):\n",
    "                if label == \"ok\":\n",
    "                    mrr = 1/(i+1)\n",
    "                    break\n",
    "                \n",
    "\n",
    "            correct = [l == \"ok\" for l in labels]\n",
    "            precision = np.mean(correct)\n",
    "\n",
    "            if len(labels) == 2:\n",
    "                failed1 = 1.0 if labels[0] == 'bad' and labels[1] == 'ok' else 0.0\n",
    "                failed2 = 1.0 if labels[0] == 'ok' and labels[1] == 'bad' else 0.0\n",
    "                bothok = 1.0 if labels[0] == 'ok' and labels[1] == 'ok' else 0.0\n",
    "                bothfail = 1.0 if labels[0] == 'bad' and labels[1] == 'bad' else 0.0\n",
    "            else:\n",
    "                failed1 = 1.0 if labels[0] == \"bad\" else 0.0\n",
    "                failed2 = np.nan\n",
    "                bothok = np.nan\n",
    "                bothfail = np.nan\n",
    "            \n",
    "            method2mrr[method].append(mrr)\n",
    "            method2p[method].append(precision)\n",
    "            method2failed1[method].append(failed1)\n",
    "            method2failed2[method].append(failed2)\n",
    "            method2bothok[method].append(bothok)\n",
    "            method2bothfail[method].append(bothfail)\n",
    "\n",
    "        for method in sorted(method2mrr.keys()):\n",
    "            n = len(method2mrr[method])\n",
    "            mrr2 =  100*np.mean(method2mrr[method])\n",
    "            p2 =  100*np.mean(method2p[method])\n",
    "            failed1 =  100*np.mean(method2failed1[method])\n",
    "            failed2 =  100*np.mean(method2failed2[method])\n",
    "            bothok =  100*np.mean(method2bothok[method])\n",
    "            bothfail =  100*np.mean(method2bothfail[method])\n",
    "            results.append({\n",
    "                \"model\": model,\n",
    "                \"method\": method, \n",
    "                \"n\": n,\n",
    "                \"lead\": par1fraction, # percent of leading (1st paragraphs) retrieved\n",
    "                \"MRR@2\": mrr2, \n",
    "                \"P@2\": p2, \n",
    "                # \"both_ok\": bothok, \n",
    "                # \"both_fail\": bothfail, \n",
    "                # \"failed1\":failed1, \n",
    "                # \"failed2\": failed2\n",
    "                })\n",
    "    if return_df:\n",
    "        return pd.DataFrame(results)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>n</th>\n",
       "      <th>lead</th>\n",
       "      <th>MRR@2</th>\n",
       "      <th>P@2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>anserini</td>\n",
       "      <td>100</td>\n",
       "      <td>34.125000</td>\n",
       "      <td>90.5</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert</td>\n",
       "      <td>100</td>\n",
       "      <td>34.125000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>68.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert_ans</td>\n",
       "      <td>100</td>\n",
       "      <td>34.125000</td>\n",
       "      <td>94.0</td>\n",
       "      <td>73.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert_nli</td>\n",
       "      <td>100</td>\n",
       "      <td>34.125000</td>\n",
       "      <td>81.0</td>\n",
       "      <td>65.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert</td>\n",
       "      <td>100</td>\n",
       "      <td>66.833333</td>\n",
       "      <td>65.0</td>\n",
       "      <td>44.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert_ans</td>\n",
       "      <td>100</td>\n",
       "      <td>66.833333</td>\n",
       "      <td>76.5</td>\n",
       "      <td>55.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert_nli</td>\n",
       "      <td>100</td>\n",
       "      <td>66.833333</td>\n",
       "      <td>63.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model       method    n       lead  MRR@2   P@2\n",
       "0        NA     anserini  100  34.125000   90.5  71.0\n",
       "1  QACG-SUM      colbert  100  34.125000   95.0  68.5\n",
       "2  QACG-SUM  colbert_ans  100  34.125000   94.0  73.0\n",
       "3  QACG-SUM  colbert_nli  100  34.125000   81.0  65.5\n",
       "4     FEVER      colbert  100  66.833333   65.0  44.5\n",
       "5     FEVER  colbert_ans  100  66.833333   76.5  55.5\n",
       "6     FEVER  colbert_nli  100  66.833333   63.0  46.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DIR = \"fever2fullwiki\" # FEVER testing, QACG-SUM model, QACG corpora\n",
    "# DIR = \"feverfever2fullwiki\" # FEVER testing, FEVER model, QACG corpora\n",
    "# DIR = \"fullwiki2fullwiki\" # QACG testing, QACG-SUM model, QACG corpora\n",
    "# DIR = \"fullwikifever2fullwiki\" # QACG testing, FEVER model, QACG corpora\n",
    "\n",
    "df = get_annotation_stats({\n",
    "    \"QACG-SUM\": {\"file\": \"data/ncaa/er_annotations/fullwiki2fullwiki/en_annotations.jsonl\"},\n",
    "    \"FEVER\": {\"file\": \"data/ncaa/er_annotations/fullwikifever2fullwiki/en_annotations.jsonl\"},\n",
    "    }, ignore_leading=False)\n",
    "\n",
    "df.loc[df.method == \"anserini\",\"model\"] = \"NA\"\n",
    "# df.sort_values(\"MRR@2\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>n</th>\n",
       "      <th>lead</th>\n",
       "      <th>MRR@2</th>\n",
       "      <th>P@2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>anserini</td>\n",
       "      <td>100</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>86.5</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert</td>\n",
       "      <td>100</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>86.5</td>\n",
       "      <td>63.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert_ans</td>\n",
       "      <td>100</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>89.0</td>\n",
       "      <td>66.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert_nli</td>\n",
       "      <td>100</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>84.0</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert</td>\n",
       "      <td>100</td>\n",
       "      <td>72.666667</td>\n",
       "      <td>50.0</td>\n",
       "      <td>33.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert_ans</td>\n",
       "      <td>100</td>\n",
       "      <td>72.666667</td>\n",
       "      <td>54.5</td>\n",
       "      <td>38.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert_nli</td>\n",
       "      <td>100</td>\n",
       "      <td>72.666667</td>\n",
       "      <td>50.0</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model       method    n       lead  MRR@2   P@2\n",
       "0        NA     anserini  100  50.000000   86.5  66.0\n",
       "1  QACG-SUM      colbert  100  50.000000   86.5  63.5\n",
       "2  QACG-SUM  colbert_ans  100  50.000000   89.0  66.5\n",
       "3  QACG-SUM  colbert_nli  100  50.000000   84.0  63.0\n",
       "4     FEVER      colbert  100  72.666667   50.0  33.5\n",
       "5     FEVER  colbert_ans  100  72.666667   54.5  38.5\n",
       "6     FEVER  colbert_nli  100  72.666667   50.0  34.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_annotation_stats({\n",
    "    \"QACG-SUM\": {\"file\": \"data/ncaa/er_annotations/fullwiki2fullwiki/cs_annotations.jsonl\"},\n",
    "    \"FEVER\": {\"file\": \"data/ncaa/er_annotations/fullwikifever2fullwiki/cs_annotations.jsonl\"},\n",
    "    }, ignore_leading=False)\n",
    "\n",
    "df.loc[df.method == \"anserini\",\"model\"] = \"NA\"\n",
    "# df.sort_values(\"MRR@2\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>n</th>\n",
       "      <th>lead</th>\n",
       "      <th>MRR@2</th>\n",
       "      <th>P@2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>anserini</td>\n",
       "      <td>100</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>67.5</td>\n",
       "      <td>57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert</td>\n",
       "      <td>100</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>60.0</td>\n",
       "      <td>49.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert_ans</td>\n",
       "      <td>100</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>66.5</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert_nli</td>\n",
       "      <td>100</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert</td>\n",
       "      <td>100</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>72.0</td>\n",
       "      <td>56.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert_ans</td>\n",
       "      <td>100</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>74.5</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert_nli</td>\n",
       "      <td>100</td>\n",
       "      <td>73.333333</td>\n",
       "      <td>65.0</td>\n",
       "      <td>52.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model       method    n       lead  MRR@2   P@2\n",
       "0        NA     anserini  100  43.750000   67.5  57.0\n",
       "1  QACG-SUM      colbert  100  43.750000   60.0  49.5\n",
       "2  QACG-SUM  colbert_ans  100  43.750000   66.5  58.0\n",
       "3  QACG-SUM  colbert_nli  100  43.750000   57.0  46.0\n",
       "4     FEVER      colbert  100  73.333333   72.0  56.5\n",
       "5     FEVER  colbert_ans  100  73.333333   74.5  60.0\n",
       "6     FEVER  colbert_nli  100  73.333333   65.0  52.5"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_annotation_stats({\n",
    "    \"QACG-SUM\": {\"file\": \"data/ncaa/er_annotations/fever2fullwiki/en_annotations.jsonl\"},\n",
    "    \"FEVER\": {\"file\": \"data/ncaa/er_annotations/feverfever2fullwiki/en_annotations.jsonl\"},\n",
    "    }, ignore_leading=False)\n",
    "\n",
    "df.loc[df.method == \"anserini\",\"model\"] = \"NA\"\n",
    "# df.sort_values(\"MRR@2\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>method</th>\n",
       "      <th>n</th>\n",
       "      <th>lead</th>\n",
       "      <th>MRR@2</th>\n",
       "      <th>P@2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NA</td>\n",
       "      <td>anserini</td>\n",
       "      <td>100</td>\n",
       "      <td>46.750000</td>\n",
       "      <td>61.0</td>\n",
       "      <td>46.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert</td>\n",
       "      <td>100</td>\n",
       "      <td>46.750000</td>\n",
       "      <td>52.5</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert_ans</td>\n",
       "      <td>100</td>\n",
       "      <td>46.750000</td>\n",
       "      <td>54.5</td>\n",
       "      <td>41.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QACG-SUM</td>\n",
       "      <td>colbert_nli</td>\n",
       "      <td>100</td>\n",
       "      <td>46.750000</td>\n",
       "      <td>46.5</td>\n",
       "      <td>31.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert</td>\n",
       "      <td>100</td>\n",
       "      <td>75.833333</td>\n",
       "      <td>78.5</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert_ans</td>\n",
       "      <td>100</td>\n",
       "      <td>75.833333</td>\n",
       "      <td>75.5</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FEVER</td>\n",
       "      <td>colbert_nli</td>\n",
       "      <td>100</td>\n",
       "      <td>75.833333</td>\n",
       "      <td>59.5</td>\n",
       "      <td>40.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model       method    n       lead  MRR@2   P@2\n",
       "0        NA     anserini  100  46.750000   61.0  46.5\n",
       "1  QACG-SUM      colbert  100  46.750000   52.5  37.0\n",
       "2  QACG-SUM  colbert_ans  100  46.750000   54.5  41.5\n",
       "3  QACG-SUM  colbert_nli  100  46.750000   46.5  31.5\n",
       "4     FEVER      colbert  100  75.833333   78.5  49.0\n",
       "5     FEVER  colbert_ans  100  75.833333   75.5  49.0\n",
       "6     FEVER  colbert_nli  100  75.833333   59.5  40.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_annotation_stats({\n",
    "    \"QACG-SUM\": {\"file\": \"data/ncaa/er_annotations/fever2fullwiki/cs_annotations.jsonl\"},\n",
    "    \"FEVER\": {\"file\": \"data/ncaa/er_annotations/feverfever2fullwiki/cs_annotations.jsonl\"},\n",
    "    }, ignore_leading=False)\n",
    "\n",
    "df.loc[df.method == \"anserini\",\"model\"] = \"NA\"\n",
    "# df.sort_values(\"MRR@2\", ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inconsistencies for: data/ncaa/er_annotations/fullwiki2fullwiki/en_annotations.jsonl\n",
      "inconsistencies for: data/ncaa/er_annotations/fullwikifever2fullwiki/en_annotations.jsonl\n",
      "found 0 inconsistencies\n"
     ]
    }
   ],
   "source": [
    "def check_inconsistencies(annotation_files, claim2evidence_files):\n",
    "    claim_bid_cid2name_label = defaultdict(list)\n",
    "    c2e = {c: read_json(e) for c, e in claim2evidence_files.items()}\n",
    "\n",
    "    for annotation_file in annotation_files:\n",
    "        print(f\"inconsistencies for: {annotation_file}\")\n",
    "        res = read_jsonl(annotation_file)\n",
    "\n",
    "        # find all (claim, bid) duplicities\n",
    "        for r in res:\n",
    "            name, cid = r[\"claim_id\"].split(\":\")\n",
    "            if cid not in c2e[name]:\n",
    "                # print(f\"missing {cid} for {name}\")\n",
    "                continue\n",
    "            claim = c2e[name][cid][0][\"claim\"]\n",
    "            bid = r[\"bid\"]\n",
    "            claim_bid_cid = (claim, bid, cid)\n",
    "            claim_bid_cid2name_label[claim_bid_cid].append((name, r[\"label\"]))\n",
    "\n",
    "    # find if the label in all (claim, bid) dupes is consistent\n",
    "    cnt = 0\n",
    "    for claim_bid_cid, name_label in claim_bid_cid2name_label.items():\n",
    "        label_set = set([label for name, label in name_label])\n",
    "        claim, bid, cid = claim_bid_cid\n",
    "        if len(label_set) > 1:\n",
    "            for name, label in name_label:\n",
    "                print(f\"\\t{label}: {name}:{cid}\")\n",
    "            # ev = c2e[name][cid][0]\n",
    "            for ev in c2e[name][cid]:\n",
    "                if ev[\"bid\"] == bid:\n",
    "                    break\n",
    "            print(f'\"{claim}\"')\n",
    "            print(f\"bid: {bid}\\n\")\n",
    "            print(textwrap.fill(ev[\"context\"]))\n",
    "            if ev[\"bid\"] != bid:\n",
    "                for x in c2e[name][cid]:\n",
    "                    print(\"| \", x[\"bid\"])\n",
    "                print(f\"claim_bid_cid: {claim_bid_cid}\")\n",
    "                print(f\"ev: {ev['bid']}\")\n",
    "                print(f\"label_set: {label_set}\")\n",
    "                raise RuntimeError(\"BID missmatch\")\n",
    "            print(\"----------------------------------\")\n",
    "            cnt += 1\n",
    "    print(f\"found {cnt} inconsistencies\")\n",
    "        \n",
    "\n",
    "# LANG = \"cs\"\n",
    "LANG = \"en\"\n",
    "\n",
    "# DIR = \"fever2fullwiki\" # FEVER testing, QACG-SUM model, QACG corpora\n",
    "# DIR = \"feverfever2fullwiki\" # FEVER testing, FEVER model, QACG corpora\n",
    "# DIR = \"fullwiki2fullwiki\" # QACG testing, QACG-SUM model, QACG corpora\n",
    "DIR = \"fullwikifever2fullwiki\" # QACG testing, FEVER model, QACG corpora\n",
    "\n",
    "claim2evidence_files = {\n",
    "    \"anserini\": f\"data/ncaa/er_annotations/{DIR}/{LANG}_claim2evidence_anserini.json\",\n",
    "    \"colbert\":  f\"data/ncaa/er_annotations/{DIR}/{LANG}_claim2evidence_colbert.json\",\n",
    "    \"colbert_ans\": f\"data/ncaa/er_annotations/{DIR}/{LANG}_claim2evidence_colbert_anserini_filtered.json\",\n",
    "    \"colbert_nli\": f\"data/ncaa/er_annotations/{DIR}/{LANG}_claim2evidence_colbert_nli_sorted.json\"\n",
    "}\n",
    "\n",
    "if DIR in [\"fullwiki2fullwiki\", \"fullwikifever2fullwiki\"]:\n",
    "    check_inconsistencies([\n",
    "        f\"data/ncaa/er_annotations/fullwiki2fullwiki/{LANG}_annotations.jsonl\",\n",
    "        f\"data/ncaa/er_annotations/fullwikifever2fullwiki/{LANG}_annotations.jsonl\",\n",
    "        ], claim2evidence_files)\n",
    "else:\n",
    "    check_inconsistencies([\n",
    "        f\"data/ncaa/er_annotations/fever2fullwiki/{LANG}_annotations.jsonl\",\n",
    "        f\"data/ncaa/er_annotations/feverfever2fullwiki/{LANG}_annotations.jsonl\"\n",
    "        ], claim2evidence_files)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hflarge",
   "language": "python",
   "name": "hflarge"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
